{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "524fea59",
   "metadata": {},
   "source": [
    "#### Functional APIS in keras and how to use them \n",
    "\n",
    "this notebook will have the implementaion of a keras model that is done in the functional model, for the reason that it simplifies building complex architicture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69483b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries \n",
    "import tensorflow as tf \n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='tensorflow') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "989bff9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KerasTensor shape=(None, 30), dtype=float32, sparse=False, ragged=False, name=keras_tensor_12>\n",
      "<bound method Model.summary of <Functional name=functional_3, built=True>>\n"
     ]
    }
   ],
   "source": [
    "## step by step building of the network \n",
    "\n",
    "# step 1: define the input layer for the network  \n",
    "\n",
    "input_layer = Input(shape=(30,)) # this means that the input of the network is expected to be a 20 feature vector to be processed by the network \n",
    "\n",
    "print(input_layer)\n",
    "\n",
    "# step 2: add the hidden layers \n",
    "\n",
    "hidden_1 = Dense(128, activation=\"relu\")(input_layer) # each hidden layer in the functional api uses the output of the last layer as the input for the next one. \n",
    "hidden_2 = Dense(64, activation= \"relu\")(hidden_1)# the hidden network will only consist of 128 neuron for the first layer and 64 for the second with an activation func of relu to avoid the vanishing gradient dilemma.\n",
    "\n",
    "# step 3: add the last layer (the output layer)\n",
    "\n",
    "output_layer = Dense(1, activation=\"sigmoid\")(hidden_2) # the output layer only has 1 neuron because we only expect 1 output \n",
    "\n",
    "# step 4: create the model \n",
    "\n",
    "model = Model(inputs=input_layer, outputs= output_layer)\n",
    "\n",
    "print(model.summary)\n",
    "\n",
    "# step 5: compile the model  \n",
    "\n",
    "model.compile(optimizer=\"adam\", loss =\"binary_crossentropy\", metrics= [\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b92e7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after creating the model we have to train it \n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# load in the data \n",
    "df= load_breast_cancer()\n",
    "X = df.data\n",
    "y = df.target \n",
    "\n",
    "# stanardize the data \n",
    "scale = StandardScaler()\n",
    "X_scaled = scale.fit_transform(X)\n",
    "\n",
    "# split it \n",
    "X_train , X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2 , random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce5c0850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8989 - loss: 0.2973 - val_accuracy: 0.9912 - val_loss: 0.0989\n",
      "Epoch 2/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9670 - loss: 0.0943 - val_accuracy: 0.9649 - val_loss: 0.0772\n",
      "Epoch 3/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9780 - loss: 0.0656 - val_accuracy: 0.9825 - val_loss: 0.0608\n",
      "Epoch 4/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9868 - loss: 0.0526 - val_accuracy: 0.9737 - val_loss: 0.0664\n",
      "Epoch 5/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9890 - loss: 0.0433 - val_accuracy: 0.9737 - val_loss: 0.0690\n",
      "Epoch 6/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9912 - loss: 0.0385 - val_accuracy: 0.9737 - val_loss: 0.0686\n",
      "Epoch 7/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9934 - loss: 0.0310 - val_accuracy: 0.9737 - val_loss: 0.0646\n",
      "Epoch 8/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9934 - loss: 0.0277 - val_accuracy: 0.9825 - val_loss: 0.0766\n",
      "Epoch 9/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9934 - loss: 0.0243 - val_accuracy: 0.9649 - val_loss: 0.0876\n",
      "Epoch 10/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9978 - loss: 0.0199 - val_accuracy: 0.9737 - val_loss: 0.0717\n",
      "Epoch 11/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9912 - loss: 0.0178 - val_accuracy: 0.9737 - val_loss: 0.0857\n",
      "Epoch 12/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9934 - loss: 0.0151 - val_accuracy: 0.9737 - val_loss: 0.0757\n",
      "Epoch 13/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9978 - loss: 0.0123 - val_accuracy: 0.9737 - val_loss: 0.0825\n",
      "Epoch 14/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0087 - val_accuracy: 0.9737 - val_loss: 0.0815\n",
      "Epoch 15/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9978 - loss: 0.0079 - val_accuracy: 0.9737 - val_loss: 0.0947\n",
      "Epoch 16/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0062 - val_accuracy: 0.9737 - val_loss: 0.0936\n",
      "Epoch 17/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0052 - val_accuracy: 0.9737 - val_loss: 0.0951\n",
      "Epoch 18/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0053 - val_accuracy: 0.9737 - val_loss: 0.0972\n",
      "Epoch 19/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0041 - val_accuracy: 0.9737 - val_loss: 0.0983\n",
      "Epoch 20/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 0.9737 - val_loss: 0.1088\n",
      "Epoch 21/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.9737 - val_loss: 0.1050\n",
      "Epoch 22/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.9737 - val_loss: 0.1022\n",
      "Epoch 23/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.9737 - val_loss: 0.1079\n",
      "Epoch 24/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 0.9737 - val_loss: 0.1088\n",
      "Epoch 25/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.9737 - val_loss: 0.1099\n",
      "Epoch 26/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.9737 - val_loss: 0.1139\n",
      "Epoch 27/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.9737 - val_loss: 0.1162\n",
      "Epoch 28/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.9737 - val_loss: 0.1158\n",
      "Epoch 29/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.9737 - val_loss: 0.1207\n",
      "Epoch 30/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 9.6251e-04 - val_accuracy: 0.9737 - val_loss: 0.1210\n",
      "Epoch 31/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 9.6810e-04 - val_accuracy: 0.9737 - val_loss: 0.1235\n",
      "Epoch 32/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 7.5034e-04 - val_accuracy: 0.9737 - val_loss: 0.1224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x111d962a8d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model \n",
    "\n",
    "model.fit(X_train , y_train, epochs= 32, batch_size=10, validation_data= (X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90f0a3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9737 - loss: 0.1224\n",
      "the loss is 0.12241563946008682\n",
      " the accuracy is 0.9736841917037964\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"the loss is {loss}\\n\",f\"the accuracy is {accuracy}\" )\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd416040",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
