{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "524fea59",
   "metadata": {},
   "source": [
    "#### Functional APIS in keras and how to use them \n",
    "\n",
    "this notebook will have the implementaion of a keras model that is done in the functional model, for the reason that it simplifies building complex architicture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69483b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries \n",
    "import tensorflow as tf \n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='tensorflow') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "989bff9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KerasTensor shape=(None, 30), dtype=float32, sparse=False, ragged=False, name=keras_tensor>\n",
      "<bound method Model.summary of <Functional name=functional, built=True>>\n"
     ]
    }
   ],
   "source": [
    "## step by step building of the network \n",
    "\n",
    "# step 1: define the input layer for the network  \n",
    "\n",
    "input_layer = Input(shape=(30,)) # this means that the input of the network is expected to be a 20 feature vector to be processed by the network \n",
    "\n",
    "print(input_layer)\n",
    "\n",
    "# step 2: add the hidden layers \n",
    "\n",
    "hidden_1 = Dense(128, activation=\"relu\")(input_layer) # each hidden layer in the functional api uses the output of the last layer as the input for the next one. \n",
    "hidden_2 = Dense(64, activation= \"relu\")(hidden_1)# the hidden network will only consist of 128 neuron for the first layer and 64 for the second with an activation func of relu to avoid the vanishing gradient dilemma.\n",
    "\n",
    "# step 3: add the last layer (the output layer)\n",
    "\n",
    "output_layer = Dense(1, activation=\"sigmoid\")(hidden_2) # the output layer only has 1 neuron because we only expect 1 output \n",
    "\n",
    "# step 4: create the model \n",
    "\n",
    "model = Model(inputs=input_layer, outputs= output_layer)\n",
    "\n",
    "print(model.summary)\n",
    "\n",
    "# step 5: compile the model  \n",
    "\n",
    "model.compile(optimizer=\"adam\", loss =\"binary_crossentropy\", metrics= [\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b92e7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after creating the model we have to train it \n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# load in the data \n",
    "df= load_breast_cancer()\n",
    "X = df.data\n",
    "y = df.target \n",
    "\n",
    "# stanardize the data \n",
    "scale = StandardScaler()\n",
    "X_scaled = scale.fit_transform(X)\n",
    "\n",
    "# split it \n",
    "X_train , X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2 , random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce5c0850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.9363 - loss: 0.2288 - val_accuracy: 0.9737 - val_loss: 0.0818\n",
      "Epoch 2/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9758 - loss: 0.0829 - val_accuracy: 0.9737 - val_loss: 0.0672\n",
      "Epoch 3/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9846 - loss: 0.0595 - val_accuracy: 0.9737 - val_loss: 0.0689\n",
      "Epoch 4/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9868 - loss: 0.0478 - val_accuracy: 0.9737 - val_loss: 0.0604\n",
      "Epoch 5/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9912 - loss: 0.0392 - val_accuracy: 0.9649 - val_loss: 0.0658\n",
      "Epoch 6/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9934 - loss: 0.0328 - val_accuracy: 0.9737 - val_loss: 0.0602\n",
      "Epoch 7/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9934 - loss: 0.0284 - val_accuracy: 0.9737 - val_loss: 0.0678\n",
      "Epoch 8/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9934 - loss: 0.0235 - val_accuracy: 0.9825 - val_loss: 0.0729\n",
      "Epoch 9/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9956 - loss: 0.0170 - val_accuracy: 0.9649 - val_loss: 0.0712\n",
      "Epoch 10/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9934 - loss: 0.0155 - val_accuracy: 0.9737 - val_loss: 0.0707\n",
      "Epoch 11/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9956 - loss: 0.0138 - val_accuracy: 0.9649 - val_loss: 0.0803\n",
      "Epoch 12/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9978 - loss: 0.0099 - val_accuracy: 0.9649 - val_loss: 0.0778\n",
      "Epoch 13/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9978 - loss: 0.0082 - val_accuracy: 0.9737 - val_loss: 0.0884\n",
      "Epoch 14/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9956 - loss: 0.0079 - val_accuracy: 0.9649 - val_loss: 0.0825\n",
      "Epoch 15/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9978 - loss: 0.0082 - val_accuracy: 0.9737 - val_loss: 0.0806\n",
      "Epoch 16/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9978 - loss: 0.0060 - val_accuracy: 0.9649 - val_loss: 0.0929\n",
      "Epoch 17/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0048 - val_accuracy: 0.9649 - val_loss: 0.0997\n",
      "Epoch 18/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.9649 - val_loss: 0.0954\n",
      "Epoch 19/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 0.9737 - val_loss: 0.1007\n",
      "Epoch 20/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.9649 - val_loss: 0.1018\n",
      "Epoch 21/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.9649 - val_loss: 0.1066\n",
      "Epoch 22/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.9649 - val_loss: 0.1033\n",
      "Epoch 23/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.9649 - val_loss: 0.1071\n",
      "Epoch 24/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.9649 - val_loss: 0.1092\n",
      "Epoch 25/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.9649 - val_loss: 0.1110\n",
      "Epoch 26/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 9.6784e-04 - val_accuracy: 0.9649 - val_loss: 0.1135\n",
      "Epoch 27/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.9737 - val_loss: 0.1166\n",
      "Epoch 28/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.9649 - val_loss: 0.1191\n",
      "Epoch 29/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 8.4754e-04 - val_accuracy: 0.9649 - val_loss: 0.1172\n",
      "Epoch 30/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 6.9295e-04 - val_accuracy: 0.9649 - val_loss: 0.1221\n",
      "Epoch 31/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 5.8649e-04 - val_accuracy: 0.9649 - val_loss: 0.1217\n",
      "Epoch 32/32\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 6.4201e-04 - val_accuracy: 0.9649 - val_loss: 0.1238\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x215354ff250>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model \n",
    "\n",
    "model.fit(X_train , y_train, epochs= 32, batch_size=10, validation_data= (X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90f0a3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9649 - loss: 0.1238\n",
      "the loss is 0.12384747713804245\n",
      " the accuracy is 0.9649122953414917\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"the loss is {loss}\\n\",f\"the accuracy is {accuracy}\" )\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd416040",
   "metadata": {},
   "source": [
    "#### Dropout layer and batch normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aacaa494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KerasTensor shape=(None, 30), dtype=float32, sparse=False, ragged=False, name=keras_tensor_9>\n",
      "<bound method Model.summary of <Functional name=functional_2, built=True>>\n"
     ]
    }
   ],
   "source": [
    "# lets start with the dropout layer its a normalization technique that helps prevent overfitting to improve the performance of a neural network \n",
    "# how does it work: it sets random neurons top zero so the model doesnt overly rely on specfic neurons\n",
    "# example implementation: \n",
    "\n",
    "## step by step building of the network \n",
    "from keras.layers import Dropout \n",
    "\n",
    "# step 1: define the input layer for the network  \n",
    "\n",
    "input_layer = Input(shape=(30,)) # this means that the input of the network is expected to be a 20 feature vector to be processed by the network \n",
    "\n",
    "print(input_layer)\n",
    "\n",
    "# step 2: add the hidden layers \n",
    "\n",
    "hidden_1 = Dense(128, activation=\"relu\")(input_layer) # each hidden layer in the functional api uses the output of the last layer as the input for the next one. \n",
    "\n",
    "Dropout_layer = Dropout(0.5) (hidden_1)\n",
    "\n",
    "hidden_2 = Dense(64, activation= \"relu\")(hidden_1)# the hidden network will only consist of 128 neuron for the first layer and 64 for the second with an activation func of relu to avoid the vanishing gradient dilemma.\n",
    "\n",
    "# step 3: add the last layer (the output layer)\n",
    "\n",
    "output_layer = Dense(1, activation=\"sigmoid\")(hidden_2) # the output layer only has 1 neuron because we only expect 1 output \n",
    "\n",
    "# step 4: create the model \n",
    "\n",
    "model = Model(inputs=input_layer, outputs= output_layer)\n",
    "\n",
    "print(model.summary)\n",
    "\n",
    "# step 5: compile the model  \n",
    "\n",
    "model.compile(optimizer=\"adam\", loss =\"binary_crossentropy\", metrics= [\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "389e46b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m1,344\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,825</span> (22.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,825\u001b[0m (22.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,697</span> (22.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,697\u001b[0m (22.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m128\u001b[0m (512.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Batch normalization: Batch Normalization is a technique used to improve the training stability and speed of neural networks. \n",
    "# It normalizes the output of a previous layer by re-centering and re-scaling the data, which helps in stabilizing the learning process.\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(20,))\n",
    "\n",
    "# Add a hidden layer\n",
    "hidden_layer = Dense(64, activation='relu')(input_layer)\n",
    "\n",
    "# Add a BatchNormalization layer\n",
    "batch_norm_layer = BatchNormalization()(hidden_layer)\n",
    "\n",
    "# Add another hidden layer after BatchNormalization\n",
    "hidden_layer2 = Dense(64, activation='relu')(batch_norm_layer)\n",
    "\n",
    "# Define the output layer\n",
    "output_layer = Dense(1, activation='sigmoid')(hidden_layer2)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Summary of the model\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a8ae4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
